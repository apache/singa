package singa;

// To start a training job, all we need is a JobProto object.
// It should contain following fields
//  - Job Name (name)
//      the name to identify the job
//  - NeuralNet (neuralnet)
//      the neural network structure contains a set of layers
//  - Train One Batch (alg)
//      the training algorithm
//  - Updater (updater)
//      the protocol for updating parameters at server side
//  - Cluster Topology (cluster)
//      the distributed topology of workers/servers
//  - Training Steps (train_steps)
//      the number of training iteration
//  All other fields/functions are optional, e.g., test, checkpoint
//
message JobProto {
  // job name, e.g., "cifar10-dcnn", "mnist-mlp"
  required string name = 1;
  // neural net consits of a set of connected layers
  required NetProto neuralnet = 3;
  // algorithm for computing gradients over one mini-batch
  required AlgProto train_one_batch = 5;
  // configuration of SGD updater, including learning rate, etc.
  required UpdaterProto updater = 7;
  // cluster toplogy conf
  required ClusterProto cluster = 9;
  // total num of steps for training
  required int32 train_steps = 16;
  // frequency of displaying training info
  optional int32 disp_freq = 17 [default = 0];

  // frequency of test, e.g., do test every 100 training steps
  optional int32 test_freq = 20 [default = 0];
  // total num of steps for testing all test data;
  // TODO(wangwei): set -1 for test forever
  optional int32 test_steps =  21 [default = 0];
  // frequency of validation, e.g., do validation every 100 training steps
  optional int32 valid_freq = 25 [default = 0];
  // total num of steps for validating all validation data
  optional int32 valid_steps = 26 [default = 0];
  // frequency of checkpoint
  optional int32 checkpoint_freq = 30 [default = 0];

  // for loading checkpoint files to init parameters
  repeated string checkpoint_path = 60;
  // send parameters to servers after training for this num of steps
  optional int32 warmup_steps = 61 [default = 0];
  // display debug info
  optional bool debug = 62 [default = false];
  // reset the version of params loaded from checkpoint file to step
  optional bool reset_param_version = 63 [default = true];
  // set num of threads used by openblas
  optional int32 num_openblas_threads = 64 [default = 1];

  // start checkpoint after this num steps
  optional int32 checkpoint_after = 80 [default = 0];
  // start display after this num steps
  optional int32 disp_after =  81[default = 0];
  // start test after this num steps
  optional int32 test_after = 82 [default = 0];
  // start validation after this num steps
  optional int32 valid_after = 83 [default = 0];

  // for internal use
  // users typically do not touch following fields

  // resume flag
  optional bool resume = 90 [default = false];
  // last snapshot step
  optional int32 step = 91 [default = 0];
  // job id allocated by zookeeper
  optional int32 id = 92 [default = -1];

  extensions 101 to 200;
}

// Protos used by JobProto
// -----------------------

message AlgProto {
  // algorithms calculating gradients for one mini-batch/iteration
  optional AlgType alg = 1 [default = kUserAlg];
  // user defined algorithm
  optional string user_alg = 2;
  // for setting CD fields
  optional CDProto cd_conf = 10;

  extensions 101 to 200;
}
message NetProto {
  repeated LayerProto layer = 1;
  // partitioning type for parallelism
  optional int32 partition_dim = 20 [default = 0];
}

message UpdaterProto {
  // built-in updater type
  optional UpdaterType type = 1 [default = kUserUpdater];
  // user-defned updater type
  optional string user_type = 2;

  // configuration for RMSProp algorithm
  optional RMSPropProto rmsprop_conf = 3;

  // learning rate generator
  optional LRGenProto learning_rate = 11;
  optional float momentum = 31 [default = 0];
  optional float weight_decay = 32 [default = 0];

  // used to avoid divide by 0, i.e. x/(y+delta)
  optional float delta = 35 [default = 0.00000001];

  extensions 101 to 200;
}

message ClusterProto {
  optional int32 nworker_groups = 1;
  optional int32 nserver_groups = 2;
  optional int32 nworkers_per_group = 3 [default = 1];
  optional int32 nservers_per_group = 4 [default = 1];
  optional int32 nworkers_per_procs = 5 [default = 1];
  optional int32 nservers_per_procs = 6 [default = 1];
  // local workspace for checkpoint files and vis files
  required string workspace = 10;

  // servers and workers in different processes?
  optional bool server_worker_separate = 20 [default = false];

  // port number used by ZeroMQ
  optional int32 start_port = 60 [default = 6723];
  // conduct updates at server side; otherwise do it at worker side
  optional bool server_update = 61 [default = true];
  // share memory space between worker groups in one procs
  optional bool share_memory = 62 [default = true];

  // bandwidth of ethernet, Bytes per second, default is 1 Gbps
  optional int32 bandwidth = 80 [default = 134217728];
  // poll time in milliseconds
  optional int32 poll_time = 81 [default = 100];
}

message CDProto {
  //number of steps for gibbs sampling
  optional int32 cd_k = 1 [default = 1];
}

message LayerProto {
  // the layer name used for identification
  required string name = 1;
  // source layer names
  repeated string srclayers = 3;
  // parameters, e.g., weight matrix or bias vector
  repeated ParamProto param = 12;
  // all layers are included in the net structure for training phase by default.
  // some layers like data layer for loading test data are not used by training
  // phase should be removed by setting the exclude field.
  repeated Phase exclude = 15;
  // type of built-in layer
  optional LayerType type = 20 [default = kUserLayer];
  // type of user layer
  optional string user_type =21;

  // proto for the specific layer
  // configuration for convolution layer
  optional ConvolutionProto convolution_conf = 30;
  // configuration for concatenation layer
  optional ConcateProto concate_conf = 31;
  // configuration for dropout layer
  optional DropoutProto dropout_conf = 33;
  // configuration for euclideanloss layer
  optional EuclideanLossProto euclideanloss_conf = 50;
  // configuration for inner product layer
  optional InnerProductProto innerproduct_conf = 34;
  // configuration for local response normalization layer
  optional DataProto lmdbdata_conf = 35;
  // configuration for local response normalization layer
  optional LRNProto lrn_conf = 45;
  // configuration for mnist parser layer
  optional MnistProto mnist_conf = 36;
  // configuration for pooling layer
  optional PoolingProto pooling_conf = 37;
  // configuration for prefetch layer
  optional PrefetchProto prefetch_conf = 44;
  // configuration for rbmhid layer
  optional RBMHidProto rbmhid_conf = 49;
  // configuration for rectified linear unit layer
  optional ReLUProto relu_conf = 38;
  // configuration for rgb image parser layer
  optional RGBImageProto rgbimage_conf = 39;
  // configuration for data layer
  optional DataProto sharddata_conf = 32;
  // configuration for slice layer
  optional SliceProto slice_conf = 41;
  // configuration for softmax loss layer
  optional SoftmaxLossProto softmaxloss_conf = 40;
  // configuration for split layer
  optional SplitProto split_conf = 42;


  // overrides the partition dimension for neural net
  optional int32 partition_dim = 60 [default = -1];
  // names of parameters shared from other layers
  optional int32 partition_id = 90 [default = 0];

  extensions 101 to 200;
}

// weight matrix should be defined before bias vector
// TODO(wangwei): separate conf for diff init method
message ParamProto {
  // used for identifying the same params from diff models and display deug info
  optional string name =  1 [default = ""];
  // for built-in Param
  optional ParamType type = 3 [default = kParam];
  // for user-defined Param
  optional string user_type = 4;

  optional ParamGenProto init =5;
    // multiplied on the global learning rate.
  optional float lr_scale = 15 [default = 1];
  // multiplied on the global weight decay.
  optional float wd_scale = 16 [default = 1];

  // name of the owner param from which this param shares the values
  optional string share_from = 60;

  // used interally
  optional int32 id = 90;
  // used internally
  optional int32 owner = 91 [default = -1];
  // partition dimension, -1 for no partition
  optional int32 partition_dim = 92;
  // usually, the program will infer the param shape
  repeated int32 shape = 93;

  extensions 101 to 200;
}

// ---------------------------
// protos for different layers
// ---------------------------
// learning rate generator proto
message LRGenProto {
  // user-defined change method
  optional ChangeMethod type = 1 [default = kUserChange];
  optional string user_type = 2;

  optional float base_lr = 3 [default = 0.01];

  optional FixedStepProto fixedstep_conf = 40;
  optional StepProto step_conf = 41;
  optional LinearProto linear_conf = 42;
  optional ExponentialProto exponential_conf = 43;
  optional InverseProto inverse_conf = 44;
  optional InverseTProto inverset_conf = 45;

  extensions 101 to 200;
}

message ParamGenProto {
  optional InitMethod type = 1 [default = kUserInit];
  optional string user_type =2;
  // constant init
  optional float value = 3 [default = 1];
  // for gaussian sampling
  optional float mean = 4 [default = 0];
  optional float std = 5 [default = 1];
  // for uniform sampling
  optional float low = 8 [default = -1];
  optional float high = 9 [default = 1];

  extensions 101 to 200;
}

message RGBImageProto {
  // scale factor for each pixel
  optional float scale = 1 [default = 1.0];
  // size after cropping
  optional int32 cropsize = 2 [default = 0];
  // mirror the image
  optional bool mirror = 3 [default = false];
  // meanfile path
  optional string meanfile = 4 [default = ""];
}

message PrefetchProto {
  repeated LayerProto sublayers = 1;
}

message SplitProto {
  optional int32 num_splits = 1 [default = 1];
}

message EuclideanLossProto {
}

message SoftmaxLossProto {
  // computing accuracy against topk results
  optional int32 topk = 1 [default = 1];
  // loss scale factor
  optional float scale = 30 [default = 1];
}

message ConvolutionProto {
  // The number of outputs for the layer
  required int32 num_filters = 1;
  // the kernel height/width
  required int32 kernel = 2;

  // The padding height/width
  optional int32 pad = 30 [default = 0];
  // the stride
  optional int32 stride = 31 [default = 1];
  // whether to have bias terms
  optional bool bias_term = 32 [default = true];
}

message ConcateProto {
  // on which dimension, starts from 0
  required int32 concate_dim = 1;
}

message DataProto {
  // path to the data file/folder, absolute or relative to the workspace
  required string path = 2;
  // batch size.
  required int32 batchsize = 4;
  // skip [0,random_skip] records
  optional int32 random_skip = 30 [default = 0];
}

message MnistProto {
  // normalization x/norm_a
  required float norm_a = 1 [default = 1];
  // normalization x-norm_b
  required float norm_b = 2 [default = 0];

  // elastic distortion
  optional int32 kernel = 30 [default = 0];
  optional float sigma = 31 [default = 0];
  optional float alpha = 32 [default = 0];
  // rotation or horizontal shearing
  optional float beta = 33 [default = 0];
  // scaling
  optional float gamma = 34 [default = 0];
  // scale to this size as input for deformation
  optional int32 resize = 35 [default = 0] ;
  optional int32 elastic_freq = 36 [default = 0];
}

// Message that stores parameters used by DropoutLayer
message DropoutProto {
  // dropout ratio
  optional float dropout_ratio = 30 [default = 0.5];
}

message RBMHidProto {
  optional int32 hid_dim = 1; // The number of outputs for the layer
  optional bool bias_term = 2 [default = true]; // whether to have bias terms
  optional bool gaussian = 3 [default = false]; // use gaussian sampling or not
}

// Message that stores parameters used by InnerProductLayer
message InnerProductProto {
  // number of outputs for the layer
  required int32 num_output = 1;
  // use bias vector or not
  optional bool bias_term = 30 [default = true];
  // transpose or not
  optional bool transpose = 31 [default = false];
}

message LRNProto {
  // local response size
  required int32 local_size = 1 [default = 5];
  // scale factor
  optional float alpha = 31 [default = 1.0];
  // exponential number
  optional float beta = 32 [default = 0.75];
  // offset
  optional float knorm = 34 [default = 1.0];
}

message PoolingProto {
  // The kernel size (square)
  required int32 kernel= 1;
  enum PoolMethod {
    MAX = 0;
    AVE = 1;
  }
  // The pooling method
  optional PoolMethod pool = 30 [default = MAX];
  // The padding size
  optional uint32 pad = 31 [default = 0];
  // The stride
  optional uint32 stride = 32 [default = 1];
}

message SliceProto {
  required int32 slice_dim = 1;
}

message ReLUProto {
  // Ref. Maas, A. L., Hannun, A. Y., & Ng, A. Y. (2013).
  // Rectifier nonlinearities improve neural network acoustic models.
  // In ICML Workshop on Deep Learning for Audio, Speech, and Language Processing.
  optional float negative_slope = 1 [default = 0];
}

message RMSPropProto {
 // history=history*rho_+(1-rho_)*(grad*grad_scale);
  required float rho = 1;
}

message FixedStepProto {
  repeated int32 step = 28;
  // lr = step_lr[i] if current step >= step[i]
  repeated float step_lr = 29;
}

message StepProto {
  // lr = base_lr * gamma^(step/change_freq)
  required float gamma = 35 [default = 1];
  // lr = base_lr * gamma^(step/change_freq)
  required int32 change_freq = 40;
}

message LinearProto {
  // lr = (1 - step / freq) * base_lr + (step / freq) * final_lr
  required int32 change_freq= 40;
  // lr = (1 - step / freq) * base_lr + (step / freq) * final_lr
  required float final_lr = 39;
}

message ExponentialProto {
  // lr = base / 2^(step/change_freq)
  required int32 change_freq = 40;
}

message InverseTProto {
  // lr = base_lr / (1+step/final_lr)
  required float final_lr = 39;
}
message InverseProto {
  // lr = base_lr*(1+gamma*step)^(-pow)
  required float gamma = 1 [default = 1];
  // lr = base_lr*(1+gamma*step)^(-pow)
  required float pow = 2 [default = 0];
}
message UniformProto {
  optional float low = 1 [default = -1];
  optional float high = 2 [default = 1];
}
message GaussianProto {
  optional float mean = 1 [default = 0];
  optional float std = 2 [default = 1];
}


// --------------
// All Enum Types
// --------------

enum ChangeMethod {
  kFixed = 0;
  kInverseT = 1;
  kInverse = 2;
  kExponential = 3;
  kLinear = 4;
  kStep = 5;
  kFixedStep = 6;
  // For user defiend change method
  kUserChange = 100;
}

enum InitMethod {
  // fix the values of all parameters  a constant in the value field
  kConstant = 0;
  // sample gaussian with std and mean
  kGaussian = 1;
  // uniform sampling between low and high
  kUniform = 2;
  // from Toronto Convnet, let a=1/sqrt(fan_in), w*=a after generating from
  // Gaussian distribution
  kGaussianSqrtFanIn = 4;
  // from Toronto Convnet, rectified linear activation, let
  // a=sqrt(3)/sqrt(fan_in), range is [-a, +a]; no need to set value=sqrt(3),
  // the program will multiply it.
  kUniformSqrtFanIn = 5;
  // from Theano MLP tutorial, let a=sqrt(6/(fan_in+fan_out)). for tanh
  // activation, range is [-a, +a], for sigmoid activation, range is
  // [-4a, +4a], put the scale factor to value field.
  // <a href="http://deeplearning.net/tutorial/mlp.html"> Theano MLP</a>
  kUniformSqrtFanInOut = 6;

  // For user defined init method
  kUserInit = 101;
}

enum LayerType {
  // Data layers
  //  - Load records from file, database
  kLMDBData = 17;
  kPrefetch = 19;
  kShardData = 3;
  // Parser layers
  //  - Parse features from records, e.g., pixels
  kLabel = 18;
  kMnist = 7;
  kRGBImage = 10;
  // Neuron layers
  //  - Feature transformation
  kConvolution = 1;
  kDropout = 4;
  kInnerProduct = 5;
  kLRN = 6;
  kPooling = 8;
  kReLU = 9;
  kRBMVis = 23;
  kRBMHid = 24;
  kSigmoid = 26;
  kSTanh = 14;
  // Loss layers
  //  - Compute objective loss
  kSoftmaxLoss = 11;
  kEuclideanLoss = 25;
  // Connection layers
  //  - Connect layers when neural net is partitioned
  kBridgeDst = 16;
  kBridgeSrc = 15;
  kConcate = 2;
  kSlice = 12;
  kSplit = 13;

  // Indicate the user defined layer. Users should configure user_type
  kUserLayer = 102;
}

enum PartitionType {
  kDataPartition = 0;
  kLayerPartition = 1;
  kNone = 2;
}

enum Phase {
  kUnknown = 0;
  kTrain = 1;
  kValidation = 2;
  kTest= 4;
  // postivie phase for contrastive divergence algorithm
  kPositive = 8;
  // negative phase for contrastive divergence algorithm
  kNegative = 16;
  kForward = 32;
  kBackward = 64;
  kLoss = 128;
}

enum ParamType {
  // built-in Param
  kParam = 0;
  // user-defined Param
  kUser = 103;
}

enum AlgType {
  // Back-propagation algorithm for feed-forward models, e.g., CNN and RNN
  kBP = 1;
  // Contrastive Divergence algorithm for RBM, DBM, etc.
  kCD = 2;
  // For user defined algorithm.
  kUserAlg = 104;
}

enum UpdaterType {
  // noraml SGD with momentum and weight decay
  kSGD = 1;
  // adaptive subgradient, http://www.magicbroom.info/Papers/DuchiHaSi10.pdf
  kAdaGrad = 2;
  // http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf
  kRMSProp = 3;
  // Nesterov first optimal gradient method
  kNesterov = 4;
  // For user defined updater
  kUserUpdater = 105;
}
